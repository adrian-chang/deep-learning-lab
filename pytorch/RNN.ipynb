{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN\n",
    "* Main idea is like a neuron gets part of its input from a different neuron on the same level\n",
    "* This is good for series of data\n",
    "    * time series prediction\n",
    "    * text generation (sentences have a particular series)\n",
    "    * etc.\n",
    "* Extreme is have neuron to neuron to neuron then go to the neurons that produce the final input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various RNNs\n",
    "\n",
    "```\n",
    "nn.RNN # hidden_size is the secret input / output to another neuron\n",
    "nn.GRU\n",
    "nn.LSTM\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "h = [1,0,0,0]\n",
    "e = [0,1,0,0]\n",
    "l = [0,0,1,0]\n",
    "o = [0,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = t.nn.RNN(input_size=4, hidden_size=2, num_layers=3, batch_first=True, bidirectional=True) # 2 out, 2 hidden, add seq_len for length of 3rd dim\n",
    "inputs = t.Tensor([[[1, 0, 0, 0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = t.randn(6,1,2) ## depth, rows, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t1, hidden_t1 = cell(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out tensor([[[-0.2562,  0.3281,  0.9079,  0.3072]]]), hidden tensor([[[-0.8171,  0.9373]],\n",
      "\n",
      "        [[-0.6565,  0.6108]],\n",
      "\n",
      "        [[ 0.7963,  0.9485]],\n",
      "\n",
      "        [[-0.7855,  0.9595]],\n",
      "\n",
      "        [[-0.2562,  0.3281]],\n",
      "\n",
      "        [[ 0.9079,  0.3072]]])\n"
     ]
    }
   ],
   "source": [
    "print(f'out {out_t1.data}, hidden {hidden_t1.data}') \n",
    "# 3 layers, each 1 with hidden, out_1 is result of going through 3 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with multiple words ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs 2 size torch.Size([1, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# one word\n",
    "inputs_2 = t.Tensor([[\n",
    "    h,e,l,l,o\n",
    "]])\n",
    "# input_size = columns, or one hot size\n",
    "cell_2 = t.nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "print(f'inputs 2 size {inputs_2.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out tensor([[[-0.5892,  0.5320],\n",
      "         [ 0.3550, -0.7357],\n",
      "         [ 0.6484,  0.4687],\n",
      "         [ 0.4873,  0.2668],\n",
      "         [ 0.1915,  0.3272]]]) torch.Size([1, 5, 2]), hidden tensor([[[ 0.1915,  0.3272]]])\n"
     ]
    }
   ],
   "source": [
    "out, hidden = cell_2(inputs_2,  t.randn(1,1,2))\n",
    "print(f'out {out.data} {out.data.size()}, hidden {hidden.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs 3 size torch.Size([3, 5, 4])\n",
      "out tensor([[[-0.0048, -0.0478],\n",
      "         [-0.4024,  0.2319],\n",
      "         [-0.4744,  0.5936],\n",
      "         [-0.4182,  0.6447],\n",
      "         [-0.1237,  0.8420]],\n",
      "\n",
      "        [[-0.3012,  0.3575],\n",
      "         [-0.1574,  0.8211],\n",
      "         [-0.2656,  0.6674],\n",
      "         [-0.3377,  0.6501],\n",
      "         [-0.3643,  0.6493]],\n",
      "\n",
      "        [[-0.4941,  0.3444],\n",
      "         [-0.4766,  0.6116],\n",
      "         [-0.3967,  0.3792],\n",
      "         [-0.4243,  0.3316],\n",
      "         [-0.4599,  0.6083]]]) \n",
      " torch.Size([3, 5, 2]) \n",
      " hidden tensor([[[-0.1237,  0.8420],\n",
      "         [-0.3643,  0.6493],\n",
      "         [-0.4599,  0.6083]]])\n"
     ]
    }
   ],
   "source": [
    "inputs_3 = t.Tensor([\n",
    "    [h,e,l,l,o],\n",
    "    [e, o, l, l, l],\n",
    "    [l, l, e, e, l]\n",
    "])\n",
    "hidden_3 = t.randn(1,3,2) # (num_layers * num_directions, batch, hidden_size)\n",
    "cell_3 = t.nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "print(f'inputs 3 size {inputs_3.size()}')\n",
    "out, hidden = cell_3(inputs_3, hidden_3)\n",
    "print(f'out {out.data} \\n {out.data.size()} \\n hidden {hidden.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output prob of a particular letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is fed in where the batch (3rd dimension) is one entire sequence, or entire word at a time\n",
    "\n",
    "Num of sequences x seq length x length of a single sequence (data encoded for one period of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "x_data = [0,1,0,2,3,3] # 'hihell'\n",
    "one_hot_lookup = [\n",
    "    [1,0,0,0,0], # h\n",
    "    [0,1,0,0,0], # i\n",
    "    [1,0,0,0,0],\n",
    "    [0,0,1,0,0],\n",
    "    [0,0,0,1,0]\n",
    "]\n",
    "y_data = [1,0,2,3,3,4] # ihello\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "x_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "inputs = t.Tensor(x_one_hot)\n",
    "labels = t.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "input_size = 5 # one hot length\n",
    "hidden_size = 5 # LSTM output, we need to feed into next cell\n",
    "batch_size = 1 # one sentence\n",
    "sequence_length = 1 # 1 by 1\n",
    "num_layers = 1 # 1 layer rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.rnn = t.nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden()\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.view(-1, num_classes)\n",
    "        return hidden, out\n",
    "    def init_hidden(self):\n",
    "        return t.zeros(num_layers, batch_size, hidden_size) # hidden is 1 per hidden layer? or implicit cell input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1296, -0.0692,  0.3919, -0.3089,  0.2677]]) tensor([ 1])\n",
      "tensor([[ 0.1174, -0.0303,  0.4079, -0.0409,  0.3867]]) tensor([ 0])\n",
      "tensor([[ 0.3537, -0.0704,  0.3279,  0.0441,  0.2974]]) tensor([ 2])\n",
      "tensor([[ 0.2882, -0.1989,  0.3538,  0.0616,  0.2519]]) tensor([ 3])\n",
      "tensor([[ 0.2257,  0.2637, -0.0953,  0.5952,  0.0730]]) tensor([ 3])\n",
      "tensor([[ 0.0205,  0.1227, -0.1650,  0.1063,  0.2180]]) tensor([ 4])\n",
      "epoch 1, loss 9.388077735900879\n",
      "tensor([[ 0.2263,  0.2267,  0.6132, -0.0193,  0.3578]]) tensor([ 1])\n",
      "tensor([[ 0.5170, -0.0228,  0.4883,  0.1029,  0.4820]]) tensor([ 0])\n",
      "tensor([[ 0.5701, -0.1278,  0.5864,  0.5900,  0.3387]]) tensor([ 2])\n",
      "tensor([[ 0.3026, -0.3340,  0.3707,  0.5749,  0.2331]]) tensor([ 3])\n",
      "tensor([[ 0.1160,  0.1223, -0.2890,  0.8006,  0.2345]]) tensor([ 3])\n",
      "tensor([[-0.0713,  0.1850, -0.2699,  0.4226,  0.5581]]) tensor([ 4])\n",
      "epoch 2, loss 8.202107429504395\n",
      "tensor([[ 0.2905,  0.4841,  0.7464,  0.0092,  0.2525]]) tensor([ 1])\n",
      "tensor([[ 0.7712,  0.0174,  0.6741, -0.0747,  0.3936]]) tensor([ 0])\n",
      "tensor([[ 0.7839, -0.1326,  0.8361,  0.7759,  0.0562]]) tensor([ 2])\n",
      "tensor([[ 0.2972, -0.5032,  0.5448,  0.7498, -0.0732]]) tensor([ 3])\n",
      "tensor([[-0.1032, -0.0162, -0.4058,  0.8645,  0.0543]]) tensor([ 3])\n",
      "tensor([[-0.3551,  0.3692, -0.4027,  0.3262,  0.6118]]) tensor([ 4])\n",
      "epoch 3, loss 7.346545696258545\n",
      "tensor([[ 0.2832,  0.5229,  0.7391,  0.0028,  0.0584]]) tensor([ 1])\n",
      "tensor([[ 0.8340,  0.0167,  0.6509, -0.3209,  0.1822]]) tensor([ 0])\n",
      "tensor([[ 0.8308, -0.1342,  0.9030,  0.7877, -0.3201]]) tensor([ 2])\n",
      "tensor([[ 0.0509, -0.6462,  0.5219,  0.7789, -0.3956]]) tensor([ 3])\n",
      "tensor([[-0.5196,  0.0723, -0.7074,  0.8391, -0.1121]]) tensor([ 3])\n",
      "tensor([[-0.6714,  0.6450, -0.6863, -0.2660,  0.7651]]) tensor([ 4])\n",
      "epoch 4, loss 6.69871187210083\n",
      "tensor([[ 0.2516,  0.5289,  0.7083, -0.0118, -0.1211]]) tensor([ 1])\n",
      "tensor([[ 0.8628,  0.0528,  0.5659, -0.5329, -0.0167]]) tensor([ 0])\n",
      "tensor([[ 0.8547, -0.0733,  0.9351,  0.7331, -0.5521]]) tensor([ 2])\n",
      "tensor([[-0.0943, -0.7405,  0.4943,  0.8073, -0.5997]]) tensor([ 3])\n",
      "tensor([[-0.7659,  0.1028, -0.8870,  0.8443, -0.2427]]) tensor([ 3])\n",
      "tensor([[-0.8493,  0.7810, -0.8759, -0.6310,  0.8602]]) tensor([ 4])\n",
      "epoch 5, loss 6.30797004699707\n",
      "tensor([[ 0.2145,  0.5445,  0.6575, -0.0331, -0.2583]]) tensor([ 1])\n",
      "tensor([[ 0.8862,  0.1582,  0.4114, -0.6857, -0.1667]]) tensor([ 0])\n",
      "tensor([[ 0.8746,  0.0561,  0.9520,  0.5789, -0.6241]]) tensor([ 2])\n",
      "tensor([[ 0.0445, -0.7879,  0.5553,  0.8340, -0.7123]]) tensor([ 3])\n",
      "tensor([[-0.8407, -0.2025, -0.9485,  0.9253, -0.4399]]) tensor([ 3])\n",
      "tensor([[-0.9593,  0.8405, -0.9721, -0.6252,  0.8447]]) tensor([ 4])\n",
      "epoch 6, loss 6.043398857116699\n",
      "tensor([[ 0.2201,  0.5435,  0.6226, -0.0229, -0.2866]]) tensor([ 1])\n",
      "tensor([[ 0.9191,  0.0779,  0.3004, -0.7068, -0.2432]]) tensor([ 0])\n",
      "tensor([[ 0.8598,  0.0740,  0.9504,  0.5980, -0.7120]]) tensor([ 2])\n",
      "tensor([[-0.0479, -0.8469,  0.4612,  0.8835, -0.7712]]) tensor([ 3])\n",
      "tensor([[-0.9181, -0.2188, -0.9813,  0.9452, -0.4444]]) tensor([ 3])\n",
      "tensor([[-0.9756,  0.8690, -0.9888, -0.6701,  0.9008]]) tensor([ 4])\n",
      "epoch 7, loss 5.828874111175537\n",
      "tensor([[ 0.2489,  0.5433,  0.6058, -0.0253, -0.2837]]) tensor([ 1])\n",
      "tensor([[ 0.9486, -0.0844,  0.2716, -0.6881, -0.3179]]) tensor([ 0])\n",
      "tensor([[ 0.8197,  0.0346,  0.9406,  0.7162, -0.8230]]) tensor([ 2])\n",
      "tensor([[-0.3182, -0.8951,  0.2039,  0.9261, -0.8161]]) tensor([ 3])\n",
      "tensor([[-0.9699,  0.1140, -0.9945,  0.9146, -0.2383]]) tensor([ 3])\n",
      "tensor([[-0.9544,  0.8546, -0.9885, -0.8169,  0.9683]]) tensor([ 4])\n",
      "epoch 8, loss 5.69650936126709\n",
      "tensor([[ 0.1923,  0.5799,  0.5489, -0.0907, -0.3753]]) tensor([ 1])\n",
      "tensor([[ 0.9570,  0.0535,  0.1197, -0.8088, -0.3845]]) tensor([ 0])\n",
      "tensor([[ 0.8360,  0.2138,  0.9563,  0.4528, -0.8135]]) tensor([ 2])\n",
      "tensor([[ 0.0759, -0.8918,  0.4656,  0.9093, -0.8614]]) tensor([ 3])\n",
      "tensor([[-0.9588, -0.5916, -0.9950,  0.9853, -0.6591]]) tensor([ 3])\n",
      "tensor([[-0.9960,  0.9138, -0.9988, -0.5011,  0.8821]]) tensor([ 4])\n",
      "epoch 9, loss 5.579368591308594\n",
      "tensor([[ 0.2489,  0.5975,  0.5908, -0.1474, -0.3596]]) tensor([ 1])\n",
      "tensor([[ 0.9777, -0.1707,  0.3212, -0.7773, -0.4997]]) tensor([ 0])\n",
      "tensor([[ 0.7977,  0.0191,  0.9525,  0.7646, -0.9376]]) tensor([ 2])\n",
      "tensor([[-0.5139, -0.9369, -0.0588,  0.9631, -0.9128]]) tensor([ 3])\n",
      "tensor([[-0.9931,  0.3514, -0.9990,  0.8817, -0.0750]]) tensor([ 3])\n",
      "tensor([[-0.9332,  0.8425, -0.9920, -0.9165,  0.9911]]) tensor([ 4])\n",
      "epoch 10, loss 5.53748083114624\n",
      "tensor([[ 0.1930,  0.6603,  0.5700, -0.2055, -0.4398]]) tensor([ 1])\n",
      "tensor([[ 0.9819, -0.0531,  0.2776, -0.8674, -0.5409]]) tensor([ 0])\n",
      "tensor([[ 0.8334,  0.1512,  0.9687,  0.6251, -0.9456]]) tensor([ 2])\n",
      "tensor([[-0.2966, -0.9382,  0.1881,  0.9594, -0.9381]]) tensor([ 3])\n",
      "tensor([[-0.9915, -0.2217, -0.9990,  0.9707, -0.5358]]) tensor([ 3])\n",
      "tensor([[-0.9944,  0.8967, -0.9990, -0.7678,  0.9678]]) tensor([ 4])\n",
      "epoch 11, loss 5.323827743530273\n",
      "tensor([[ 0.1220,  0.7028,  0.5367, -0.2611, -0.5262]]) tensor([ 1])\n",
      "tensor([[ 0.9832,  0.0923,  0.1618, -0.9250, -0.5908]]) tensor([ 0])\n",
      "tensor([[ 0.8345,  0.2825,  0.9758,  0.3036, -0.9388]]) tensor([ 2])\n",
      "tensor([[ 0.1742, -0.9100,  0.5568,  0.9330, -0.9599]]) tensor([ 3])\n",
      "tensor([[-0.9808, -0.8740, -0.9981,  0.9973, -0.9013]]) tensor([ 3])\n",
      "tensor([[-0.9995,  0.9343, -0.9999, -0.2420,  0.8102]]) tensor([ 4])\n",
      "epoch 12, loss 5.4139204025268555\n",
      "tensor([[ 0.1747,  0.7109,  0.5790, -0.3377, -0.5365]]) tensor([ 1])\n",
      "tensor([[ 0.9903, -0.1417,  0.3893, -0.9172, -0.7194]]) tensor([ 0])\n",
      "tensor([[ 0.7859,  0.0490,  0.9738,  0.7271, -0.9864]]) tensor([ 2])\n",
      "tensor([[-0.5557, -0.9560, -0.0876,  0.9794, -0.9774]]) tensor([ 3])\n",
      "tensor([[-0.9977,  0.1617, -0.9997,  0.9252, -0.2950]]) tensor([ 3])\n",
      "tensor([[-0.9845,  0.8491, -0.9985, -0.9197,  0.9925]]) tensor([ 4])\n",
      "epoch 13, loss 5.226313591003418\n",
      "tensor([[ 0.1498,  0.7372,  0.5743, -0.3408, -0.5872]]) tensor([ 1])\n",
      "tensor([[ 0.9910, -0.1987,  0.3336, -0.9297, -0.7696]]) tensor([ 0])\n",
      "tensor([[ 0.7066,  0.1315,  0.9724,  0.7381, -0.9915]]) tensor([ 2])\n",
      "tensor([[-0.5465, -0.9547, -0.1072,  0.9759, -0.9790]]) tensor([ 3])\n",
      "tensor([[-0.9982,  0.0508, -0.9997,  0.9364, -0.3403]]) tensor([ 3])\n",
      "tensor([[-0.9907,  0.8427, -0.9991, -0.8965,  0.9914]]) tensor([ 4])\n",
      "epoch 14, loss 5.126873970031738\n",
      "tensor([[ 0.0691,  0.7728,  0.5344, -0.2885, -0.6556]]) tensor([ 1])\n",
      "tensor([[ 0.9883, -0.1244,  0.0456, -0.9501, -0.7717]]) tensor([ 0])\n",
      "tensor([[ 0.5581,  0.4507,  0.9706,  0.4278, -0.9859]]) tensor([ 2])\n",
      "tensor([[ 0.0291, -0.9041,  0.3486,  0.9022, -0.9682]]) tensor([ 3])\n",
      "tensor([[-0.9925, -0.8823, -0.9991,  0.9968, -0.9089]]) tensor([ 3])\n",
      "tensor([[-0.9998,  0.9042, -0.9999, -0.1666,  0.8392]]) tensor([ 4])\n",
      "epoch 15, loss 5.158664703369141\n",
      "tensor([[ 0.0926,  0.7727,  0.5585, -0.2614, -0.6875]]) tensor([ 1])\n",
      "tensor([[ 0.9891, -0.3699,  0.0435, -0.9351, -0.8469]]) tensor([ 0])\n",
      "tensor([[ 0.2220,  0.4635,  0.9604,  0.7045, -0.9953]]) tensor([ 2])\n",
      "tensor([[-0.3996, -0.8757, -0.2489,  0.8679, -0.9476]]) tensor([ 3])\n",
      "tensor([[-0.9982, -0.2270, -0.9997,  0.9401, -0.3384]]) tensor([ 3])\n",
      "tensor([[-0.9961,  0.8255, -0.9996, -0.7720,  0.9849]]) tensor([ 4])\n",
      "epoch 16, loss 4.961892127990723\n",
      "tensor([[ 0.0750,  0.7882,  0.5542, -0.3488, -0.7170]]) tensor([ 1])\n",
      "tensor([[ 0.9917, -0.3407,  0.1597, -0.9535, -0.8823]]) tensor([ 0])\n",
      "tensor([[ 0.3341,  0.3273,  0.9697,  0.7375, -0.9975]]) tensor([ 2])\n",
      "tensor([[-0.5114, -0.9113, -0.2618,  0.9374, -0.9787]]) tensor([ 3])\n",
      "tensor([[-0.9990, -0.2305, -0.9998,  0.9289, -0.3178]]) tensor([ 3])\n",
      "tensor([[-0.9964,  0.8023, -0.9997, -0.8044,  0.9864]]) tensor([ 4])\n",
      "epoch 17, loss 4.893014430999756\n",
      "tensor([[ 0.0116,  0.8133,  0.5199, -0.4468, -0.7502]]) tensor([ 1])\n",
      "tensor([[ 0.9932, -0.1292,  0.1856, -0.9766, -0.8892]]) tensor([ 0])\n",
      "tensor([[ 0.5505,  0.2539,  0.9802,  0.5138, -0.9970]]) tensor([ 2])\n",
      "tensor([[-0.2822, -0.9222,  0.1983,  0.9592, -0.9936]]) tensor([ 3])\n",
      "tensor([[-0.9975, -0.8501, -0.9997,  0.9920, -0.8792]]) tensor([ 3])\n",
      "tensor([[-0.9998,  0.8197, -1.0000, -0.3066,  0.8649]]) tensor([ 4])\n",
      "epoch 18, loss 4.902654647827148\n",
      "tensor([[ 0.0190,  0.8083,  0.5280, -0.5177, -0.7719]]) tensor([ 1])\n",
      "tensor([[ 0.9946, -0.2097,  0.3305, -0.9793, -0.9251]]) tensor([ 0])\n",
      "tensor([[ 0.5622,  0.0201,  0.9826,  0.7084, -0.9990]]) tensor([ 2])\n",
      "tensor([[-0.6638, -0.9470, -0.1722,  0.9854, -0.9974]]) tensor([ 3])\n",
      "tensor([[-0.9993, -0.3809, -0.9999,  0.9240, -0.4716]]) tensor([ 3])\n",
      "tensor([[-0.9986,  0.7466, -0.9998, -0.7977,  0.9778]]) tensor([ 4])\n",
      "epoch 19, loss 4.787551403045654\n",
      "tensor([[-0.0180,  0.8078,  0.5079, -0.5521, -0.7981]]) tensor([ 1])\n",
      "tensor([[ 0.9946, -0.1855,  0.2946, -0.9845, -0.9381]]) tensor([ 0])\n",
      "tensor([[ 0.5168, -0.0199,  0.9836,  0.6434, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.6689, -0.9360, -0.1529,  0.9846, -0.9984]]) tensor([ 3])\n",
      "tensor([[-0.9993, -0.5166, -0.9999,  0.9271, -0.5424]]) tensor([ 3])\n",
      "tensor([[-0.9993,  0.7111, -0.9999, -0.7379,  0.9641]]) tensor([ 4])\n",
      "epoch 20, loss 4.70549201965332\n",
      "tensor([[-0.0866,  0.8072,  0.4652, -0.5617, -0.8250]]) tensor([ 1])\n",
      "tensor([[ 0.9935, -0.0863,  0.1161, -0.9896, -0.9402]]) tensor([ 0])\n",
      "tensor([[ 0.3942,  0.0807,  0.9831,  0.2863, -0.9985]]) tensor([ 2])\n",
      "tensor([[-0.3212, -0.8461,  0.2446,  0.9505, -0.9986]]) tensor([ 3])\n",
      "tensor([[-0.9977, -0.9427, -0.9997,  0.9919, -0.9261]]) tensor([ 3])\n",
      "tensor([[-0.9999,  0.6488, -1.0000, -0.2877,  0.7945]]) tensor([ 4])\n",
      "epoch 21, loss 4.721232891082764\n",
      "tensor([[-0.0747,  0.7848,  0.4741, -0.5542, -0.8426]]) tensor([ 1])\n",
      "tensor([[ 0.9931, -0.2943,  0.1205, -0.9871, -0.9602]]) tensor([ 0])\n",
      "tensor([[ 0.0936,  0.0027,  0.9793,  0.5745, -0.9994]]) tensor([ 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achang/miniconda2/envs/deep-learning-lab/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7191, -0.8027, -0.4842,  0.9501, -0.9979]]) tensor([ 3])\n",
      "tensor([[-0.9996, -0.4529, -0.9999,  0.6707,  0.0857]]) tensor([ 3])\n",
      "tensor([[-0.9932,  0.7811, -0.9996, -0.8657,  0.9848]]) tensor([ 4])\n",
      "epoch 22, loss 4.774805545806885\n",
      "tensor([[-0.1715,  0.8075,  0.3959, -0.6290, -0.8580]]) tensor([ 1])\n",
      "tensor([[ 0.9928,  0.1083, -0.0131, -0.9952, -0.9465]]) tensor([ 0])\n",
      "tensor([[ 0.4132,  0.0616,  0.9855, -0.3090, -0.9975]]) tensor([ 2])\n",
      "tensor([[ 0.3605, -0.6199,  0.7940,  0.8721, -0.9996]]) tensor([ 3])\n",
      "tensor([[-0.9767, -0.9991, -0.9972,  0.9997, -0.9968]]) tensor([ 3])\n",
      "tensor([[-1.0000,  0.3999, -1.0000, -0.2111,  0.7210]]) tensor([ 4])\n",
      "epoch 23, loss 4.8970112800598145\n",
      "tensor([[-0.2028,  0.8131,  0.3655, -0.7036, -0.8640]]) tensor([ 1])\n",
      "tensor([[ 0.9937,  0.2815,  0.0804, -0.9972, -0.9466]]) tensor([ 0])\n",
      "tensor([[ 0.6550, -0.1088,  0.9896, -0.4723, -0.9974]]) tensor([ 2])\n",
      "tensor([[ 0.4750, -0.7076,  0.9093,  0.9369, -0.9999]]) tensor([ 3])\n",
      "tensor([[-0.9772, -0.9996, -0.9969,  0.9999, -0.9988]]) tensor([ 3])\n",
      "tensor([[-1.0000,  0.2529, -1.0000, -0.2974,  0.7555]]) tensor([ 4])\n",
      "epoch 24, loss 4.883424282073975\n",
      "tensor([[-0.2046,  0.8072,  0.3616, -0.7616, -0.8659]]) tensor([ 1])\n",
      "tensor([[ 0.9948,  0.3135,  0.2475, -0.9977, -0.9546]]) tensor([ 0])\n",
      "tensor([[ 0.7800, -0.3389,  0.9923, -0.3098, -0.9987]]) tensor([ 2])\n",
      "tensor([[ 0.1258, -0.8355,  0.8763,  0.9810, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9889, -0.9991, -0.9989,  0.9998, -0.9979]]) tensor([ 3])\n",
      "tensor([[-1.0000,  0.0981, -1.0000, -0.4350,  0.8260]]) tensor([ 4])\n",
      "epoch 25, loss 4.734969615936279\n",
      "tensor([[-0.1857,  0.7943,  0.3755, -0.8005, -0.8663]]) tensor([ 1])\n",
      "tensor([[ 0.9956,  0.2359,  0.4288, -0.9977, -0.9658]]) tensor([ 0])\n",
      "tensor([[ 0.8313, -0.5359,  0.9938,  0.1097, -0.9996]]) tensor([ 2])\n",
      "tensor([[-0.5414, -0.9369,  0.6399,  0.9955, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9970, -0.9885, -0.9998,  0.9967, -0.9768]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.0737, -1.0000, -0.5745,  0.8930]]) tensor([ 4])\n",
      "epoch 26, loss 4.558249473571777\n",
      "tensor([[-0.1509,  0.7783,  0.3959, -0.8227, -0.8673]]) tensor([ 1])\n",
      "tensor([[ 0.9963,  0.0664,  0.5794, -0.9971, -0.9761]]) tensor([ 0])\n",
      "tensor([[ 0.8341, -0.6577,  0.9940,  0.5781, -0.9999]]) tensor([ 2])\n",
      "tensor([[-0.8725, -0.9766,  0.0350,  0.9987, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9995, -0.9154, -0.9999,  0.8973, -0.4885]]) tensor([ 3])\n",
      "tensor([[-0.9998,  0.0407, -1.0000, -0.7851,  0.9681]]) tensor([ 4])\n",
      "epoch 27, loss 4.5399932861328125\n",
      "tensor([[-0.1672,  0.7669,  0.3773, -0.8474, -0.8715]]) tensor([ 1])\n",
      "tensor([[ 0.9961,  0.0791,  0.5871, -0.9976, -0.9780]]) tensor([ 0])\n",
      "tensor([[ 0.8369, -0.6904,  0.9944,  0.5428, -0.9999]]) tensor([ 2])\n",
      "tensor([[-0.8707, -0.9769,  0.1059,  0.9986, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9994, -0.9498, -0.9999,  0.9096, -0.5261]]) tensor([ 3])\n",
      "tensor([[-0.9998, -0.1792, -1.0000, -0.8065,  0.9726]]) tensor([ 4])\n",
      "epoch 28, loss 4.485976219177246\n",
      "tensor([[-0.2213,  0.7591,  0.3255, -0.8725, -0.8782]]) tensor([ 1])\n",
      "tensor([[ 0.9953,  0.2254,  0.4864, -0.9987, -0.9742]]) tensor([ 0])\n",
      "tensor([[ 0.8358, -0.6717,  0.9946,  0.1031, -0.9998]]) tensor([ 2])\n",
      "tensor([[-0.6353, -0.9481,  0.6513,  0.9959, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9973, -0.9947, -0.9998,  0.9942, -0.9617]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.6154, -1.0000, -0.7652,  0.9573]]) tensor([ 4])\n",
      "epoch 29, loss 4.413786888122559\n",
      "tensor([[-0.2584,  0.7527,  0.2787, -0.8870, -0.8856]]) tensor([ 1])\n",
      "tensor([[ 0.9944,  0.2847,  0.3787, -0.9991, -0.9710]]) tensor([ 0])\n",
      "tensor([[ 0.7996, -0.6386,  0.9938, -0.2516, -0.9997]]) tensor([ 2])\n",
      "tensor([[-0.2727, -0.8911,  0.8484,  0.9885, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9937, -0.9993, -0.9995,  0.9990, -0.9929]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.7465, -1.0000, -0.7894,  0.9618]]) tensor([ 4])\n",
      "epoch 30, loss 4.414000511169434\n",
      "tensor([[-0.2801,  0.7474,  0.2426, -0.8941, -0.8933]]) tensor([ 1])\n",
      "tensor([[ 0.9934,  0.2668,  0.2868, -0.9993, -0.9698]]) tensor([ 0])\n",
      "tensor([[ 0.7262, -0.6073,  0.9923, -0.4184, -0.9997]]) tensor([ 2])\n",
      "tensor([[-0.0917, -0.8252,  0.8842,  0.9773, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9911, -0.9997, -0.9991,  0.9994, -0.9958]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8255, -1.0000, -0.8146,  0.9673]]) tensor([ 4])\n",
      "epoch 31, loss 4.372708797454834\n",
      "tensor([[-0.2871,  0.7422,  0.2207, -0.8953, -0.9013]]) tensor([ 1])\n",
      "tensor([[ 0.9925,  0.1719,  0.2264, -0.9994, -0.9714]]) tensor([ 0])\n",
      "tensor([[ 0.6015, -0.5891,  0.9900, -0.4106, -0.9997]]) tensor([ 2])\n",
      "tensor([[-0.1814, -0.7674,  0.8413,  0.9665, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9916, -0.9997, -0.9992,  0.9988, -0.9926]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8749, -1.0000, -0.8373,  0.9724]]) tensor([ 4])\n",
      "epoch 32, loss 4.2753682136535645\n",
      "tensor([[-0.2782,  0.7365,  0.2153, -0.8905, -0.9095]]) tensor([ 1])\n",
      "tensor([[ 0.9917, -0.0109,  0.2074, -0.9993, -0.9760]]) tensor([ 0])\n",
      "tensor([[ 0.4004, -0.5862,  0.9865, -0.1942, -0.9999]]) tensor([ 2])\n",
      "tensor([[-0.5324, -0.7324,  0.5883,  0.9625, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9960, -0.9990, -0.9997,  0.9886, -0.9323]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.8970, -1.0000, -0.8613,  0.9784]]) tensor([ 4])\n",
      "epoch 33, loss 4.097552299499512\n",
      "tensor([[-0.2518,  0.7303,  0.2243, -0.8787, -0.9179]]) tensor([ 1])\n",
      "tensor([[ 0.9912, -0.2736,  0.2255, -0.9990, -0.9822]]) tensor([ 0])\n",
      "tensor([[ 0.1110, -0.5916,  0.9806,  0.2477, -1.0000]]) tensor([ 2])\n",
      "tensor([[-0.8637, -0.7217, -0.2821,  0.9667, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9995, -0.9894, -0.9999,  0.1400,  0.6771]]) tensor([ 3])\n",
      "tensor([[-0.9696,  0.4318, -0.9995, -0.9859,  0.9939]]) tensor([ 4])\n",
      "epoch 34, loss 4.935083866119385\n",
      "tensor([[-0.3722,  0.7450,  0.0819, -0.9043, -0.9166]]) tensor([ 1])\n",
      "tensor([[ 0.9870,  0.2507, -0.1967, -0.9997, -0.9499]]) tensor([ 0])\n",
      "tensor([[ 0.1985, -0.3679,  0.9789, -0.9052, -0.9985]]) tensor([ 2])\n",
      "tensor([[ 0.4187,  0.0504,  0.8885,  0.5070, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.8078, -1.0000, -0.9651,  0.9980, -0.9956]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9723, -1.0000, -0.7400,  0.9564]]) tensor([ 4])\n",
      "epoch 35, loss 4.598489761352539\n",
      "tensor([[-0.4523,  0.7571, -0.0224, -0.9218, -0.9105]]) tensor([ 1])\n",
      "tensor([[ 0.9837,  0.6010, -0.3929, -0.9999, -0.8872]]) tensor([ 0])\n",
      "tensor([[ 0.3913, -0.2240,  0.9797, -0.9864, -0.9875]]) tensor([ 2])\n",
      "tensor([[ 0.6713, -0.1571,  0.9481,  0.4487, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.7977, -1.0000, -0.9362,  0.9995, -0.9992]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9783, -1.0000, -0.7136,  0.9499]]) tensor([ 4])\n",
      "epoch 36, loss 4.798905372619629\n",
      "tensor([[-0.5039,  0.7607, -0.0942, -0.9329, -0.8998]]) tensor([ 1])\n",
      "tensor([[ 0.9825,  0.7810, -0.4500, -0.9999, -0.7907]]) tensor([ 0])\n",
      "tensor([[ 0.6108, -0.1543,  0.9817, -0.9944, -0.9633]]) tensor([ 2])\n",
      "tensor([[ 0.7897, -0.4621,  0.9735,  0.5919, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9073, -1.0000, -0.9554,  0.9999, -0.9998]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9704, -1.0000, -0.8137,  0.9697]]) tensor([ 4])\n",
      "epoch 37, loss 4.776025295257568\n",
      "tensor([[-0.5381,  0.7547, -0.1393, -0.9394, -0.8890]]) tensor([ 1])\n",
      "tensor([[ 0.9829,  0.8664, -0.4426, -1.0000, -0.6993]]) tensor([ 0])\n",
      "tensor([[ 0.7650, -0.1609,  0.9840, -0.9960, -0.9423]]) tensor([ 2])\n",
      "tensor([[ 0.8452, -0.6458,  0.9831,  0.7245, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9531, -1.0000, -0.9690,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9667, -1.0000, -0.8527,  0.9775]]) tensor([ 4])\n",
      "epoch 38, loss 4.745235443115234\n",
      "tensor([[-0.5618,  0.7405, -0.1629, -0.9432, -0.8815]]) tensor([ 1])\n",
      "tensor([[ 0.9839,  0.9090, -0.3983, -1.0000, -0.6697]]) tensor([ 0])\n",
      "tensor([[ 0.8467, -0.2622,  0.9860, -0.9960, -0.9439]]) tensor([ 2])\n",
      "tensor([[ 0.8524, -0.7362,  0.9855,  0.8273, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9722, -1.0000, -0.9773,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9657, -1.0000, -0.8718,  0.9809]]) tensor([ 4])\n",
      "epoch 39, loss 4.697662830352783\n",
      "tensor([[-0.5794,  0.7203, -0.1716, -0.9455, -0.8786]]) tensor([ 1])\n",
      "tensor([[ 0.9851,  0.9325, -0.3327, -1.0000, -0.7052]]) tensor([ 0])\n",
      "tensor([[ 0.8885, -0.4233,  0.9880, -0.9950, -0.9614]]) tensor([ 2])\n",
      "tensor([[ 0.8309, -0.7888,  0.9854,  0.8993, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9818, -1.0000, -0.9825,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9656, -1.0000, -0.8837,  0.9825]]) tensor([ 4])\n",
      "epoch 40, loss 4.6484599113464355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5939,  0.6974, -0.1730, -0.9470, -0.8794]]) tensor([ 1])\n",
      "tensor([[ 0.9860,  0.9472, -0.2598, -1.0000, -0.7687]]) tensor([ 0])\n",
      "tensor([[ 0.9135, -0.5796,  0.9899, -0.9932, -0.9779]]) tensor([ 2])\n",
      "tensor([[ 0.8000, -0.8265,  0.9847,  0.9402, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9868, -1.0000, -0.9854,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9659, -1.0000, -0.8920,  0.9831]]) tensor([ 4])\n",
      "epoch 41, loss 4.618303298950195\n",
      "tensor([[-0.6073,  0.6751, -0.1726, -0.9480, -0.8824]]) tensor([ 1])\n",
      "tensor([[ 0.9868,  0.9574, -0.1903, -1.0000, -0.8291]]) tensor([ 0])\n",
      "tensor([[ 0.9309, -0.6963,  0.9915, -0.9904, -0.9878]]) tensor([ 2])\n",
      "tensor([[ 0.7732, -0.8557,  0.9843,  0.9614, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9898, -1.0000, -0.9870,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9662, -1.0000, -0.8984,  0.9833]]) tensor([ 4])\n",
      "epoch 42, loss 4.606808662414551\n",
      "tensor([[-0.6206,  0.6563, -0.1739, -0.9486, -0.8867]]) tensor([ 1])\n",
      "tensor([[ 0.9873,  0.9651, -0.1307, -1.0000, -0.8747]]) tensor([ 0])\n",
      "tensor([[ 0.9435, -0.7733,  0.9928, -0.9868, -0.9931]]) tensor([ 2])\n",
      "tensor([[ 0.7525, -0.8779,  0.9843,  0.9728, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9918, -1.0000, -0.9879,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9663, -1.0000, -0.9037,  0.9832]]) tensor([ 4])\n",
      "epoch 43, loss 4.604751110076904\n",
      "tensor([[-0.6342,  0.6430, -0.1786, -0.9491, -0.8915]]) tensor([ 1])\n",
      "tensor([[ 0.9877,  0.9713, -0.0840, -1.0000, -0.9058]]) tensor([ 0])\n",
      "tensor([[ 0.9525, -0.8220,  0.9937, -0.9829, -0.9958]]) tensor([ 2])\n",
      "tensor([[ 0.7346, -0.8942,  0.9844,  0.9794, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9932, -1.0000, -0.9885,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9662, -1.0000, -0.9086,  0.9832]]) tensor([ 4])\n",
      "epoch 44, loss 4.60406494140625\n",
      "tensor([[-0.6481,  0.6365, -0.1875, -0.9493, -0.8965]]) tensor([ 1])\n",
      "tensor([[ 0.9880,  0.9763, -0.0509, -1.0000, -0.9259]]) tensor([ 0])\n",
      "tensor([[ 0.9586, -0.8526,  0.9943, -0.9791, -0.9972]]) tensor([ 2])\n",
      "tensor([[ 0.7158, -0.9055,  0.9845,  0.9836, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9944, -1.0000, -0.9891,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9658, -1.0000, -0.9131,  0.9831]]) tensor([ 4])\n",
      "epoch 45, loss 4.599919319152832\n",
      "tensor([[-0.6624,  0.6370, -0.2009, -0.9493, -0.9014]]) tensor([ 1])\n",
      "tensor([[ 0.9882,  0.9804, -0.0310, -1.0000, -0.9386]]) tensor([ 0])\n",
      "tensor([[ 0.9626, -0.8717,  0.9946, -0.9760, -0.9980]]) tensor([ 2])\n",
      "tensor([[ 0.6938, -0.9128,  0.9846,  0.9865, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9953, -1.0000, -0.9896,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9652, -1.0000, -0.9175,  0.9831]]) tensor([ 4])\n",
      "epoch 46, loss 4.5900092124938965\n",
      "tensor([[-0.6770,  0.6443, -0.2184, -0.9493, -0.9061]]) tensor([ 1])\n",
      "tensor([[ 0.9884,  0.9838, -0.0237, -1.0000, -0.9464]]) tensor([ 0])\n",
      "tensor([[ 0.9650, -0.8833,  0.9948, -0.9741, -0.9984]]) tensor([ 2])\n",
      "tensor([[ 0.6668, -0.9170,  0.9846,  0.9885, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9962, -1.0000, -0.9902,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9642, -1.0000, -0.9218,  0.9832]]) tensor([ 4])\n",
      "epoch 47, loss 4.573610305786133\n",
      "tensor([[-0.6919,  0.6575, -0.2397, -0.9491, -0.9105]]) tensor([ 1])\n",
      "tensor([[ 0.9887,  0.9866, -0.0281, -1.0000, -0.9509]]) tensor([ 0])\n",
      "tensor([[ 0.9660, -0.8895,  0.9947, -0.9737, -0.9987]]) tensor([ 2])\n",
      "tensor([[ 0.6337, -0.9186,  0.9844,  0.9900, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9969, -1.0000, -0.9908,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9629, -1.0000, -0.9260,  0.9834]]) tensor([ 4])\n",
      "epoch 48, loss 4.550899505615234\n",
      "tensor([[-0.7068,  0.6753, -0.2643, -0.9488, -0.9147]]) tensor([ 1])\n",
      "tensor([[ 0.9889,  0.9889, -0.0436, -1.0000, -0.9531]]) tensor([ 0])\n",
      "tensor([[ 0.9661, -0.8919,  0.9945, -0.9749, -0.9988]]) tensor([ 2])\n",
      "tensor([[ 0.5929, -0.9180,  0.9843,  0.9911, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9975, -1.0000, -0.9916,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9614, -1.0000, -0.9300,  0.9837]]) tensor([ 4])\n",
      "epoch 49, loss 4.522526741027832\n",
      "tensor([[-0.7218,  0.6961, -0.2915, -0.9485, -0.9186]]) tensor([ 1])\n",
      "tensor([[ 0.9892,  0.9908, -0.0697, -1.0000, -0.9536]]) tensor([ 0])\n",
      "tensor([[ 0.9651, -0.8913,  0.9941, -0.9773, -0.9989]]) tensor([ 2])\n",
      "tensor([[ 0.5429, -0.9154,  0.9840,  0.9919, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9981, -1.0000, -0.9924,  1.0000, -0.9999]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9596, -1.0000, -0.9340,  0.9841]]) tensor([ 4])\n",
      "epoch 50, loss 4.489339351654053\n",
      "tensor([[-0.7367,  0.7187, -0.3207, -0.9481, -0.9222]]) tensor([ 1])\n",
      "tensor([[ 0.9895,  0.9924, -0.1057, -1.0000, -0.9529]]) tensor([ 0])\n",
      "tensor([[ 0.9633, -0.8883,  0.9935, -0.9807, -0.9989]]) tensor([ 2])\n",
      "tensor([[ 0.4815, -0.9110,  0.9836,  0.9925, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9985, -1.0000, -0.9934,  1.0000, -0.9998]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9575, -1.0000, -0.9377,  0.9845]]) tensor([ 4])\n",
      "epoch 51, loss 4.45223331451416\n",
      "tensor([[-0.7515,  0.7416, -0.3514, -0.9476, -0.9256]]) tensor([ 1])\n",
      "tensor([[ 0.9898,  0.9937, -0.1510, -1.0000, -0.9513]]) tensor([ 0])\n",
      "tensor([[ 0.9606, -0.8833,  0.9926, -0.9845, -0.9988]]) tensor([ 2])\n",
      "tensor([[ 0.4066, -0.9052,  0.9832,  0.9930, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9989, -1.0000, -0.9943,  1.0000, -0.9998]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9553, -1.0000, -0.9413,  0.9850]]) tensor([ 4])\n",
      "epoch 52, loss 4.412123203277588\n",
      "tensor([[-0.7661,  0.7639, -0.3829, -0.9471, -0.9289]]) tensor([ 1])\n",
      "tensor([[ 0.9901,  0.9947, -0.2043, -1.0000, -0.9490]]) tensor([ 0])\n",
      "tensor([[ 0.9571, -0.8768,  0.9914, -0.9881, -0.9987]]) tensor([ 2])\n",
      "tensor([[ 0.3161, -0.8979,  0.9826,  0.9933, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9992, -1.0000, -0.9953,  1.0000, -0.9997]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9528, -1.0000, -0.9447,  0.9855]]) tensor([ 4])\n",
      "epoch 53, loss 4.3700103759765625\n",
      "tensor([[-0.7803,  0.7849, -0.4147, -0.9466, -0.9319]]) tensor([ 1])\n",
      "tensor([[ 0.9904,  0.9955, -0.2639, -1.0000, -0.9465]]) tensor([ 0])\n",
      "tensor([[ 0.9527, -0.8693,  0.9898, -0.9913, -0.9984]]) tensor([ 2])\n",
      "tensor([[ 0.2091, -0.8895,  0.9819,  0.9936, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9994, -1.0000, -0.9963,  1.0000, -0.9996]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9502, -1.0000, -0.9479,  0.9861]]) tensor([ 4])\n",
      "epoch 54, loss 4.327089309692383\n",
      "tensor([[-0.7941,  0.8040, -0.4462, -0.9461, -0.9348]]) tensor([ 1])\n",
      "tensor([[ 0.9907,  0.9962, -0.3277, -1.0000, -0.9439]]) tensor([ 0])\n",
      "tensor([[ 0.9475, -0.8613,  0.9877, -0.9938, -0.9981]]) tensor([ 2])\n",
      "tensor([[ 0.0870, -0.8800,  0.9810,  0.9938, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9996, -1.0000, -0.9972,  1.0000, -0.9994]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9475, -1.0000, -0.9509,  0.9866]]) tensor([ 4])\n",
      "epoch 55, loss 4.284796714782715\n",
      "tensor([[-0.8074,  0.8211, -0.4769, -0.9456, -0.9376]]) tensor([ 1])\n",
      "tensor([[ 0.9910,  0.9967, -0.3931, -1.0000, -0.9416]]) tensor([ 0])\n",
      "tensor([[ 0.9417, -0.8535,  0.9852, -0.9957, -0.9977]]) tensor([ 2])\n",
      "tensor([[-0.0464, -0.8696,  0.9799,  0.9939, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9997, -0.9999, -0.9979,  1.0000, -0.9990]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9447, -1.0000, -0.9536,  0.9871]]) tensor([ 4])\n",
      "epoch 56, loss 4.244688034057617\n",
      "tensor([[-0.8200,  0.8363, -0.5065, -0.9452, -0.9402]]) tensor([ 1])\n",
      "tensor([[ 0.9913,  0.9972, -0.4574, -1.0000, -0.9398]]) tensor([ 0])\n",
      "tensor([[ 0.9353, -0.8465,  0.9821, -0.9970, -0.9973]]) tensor([ 2])\n",
      "tensor([[-0.1843, -0.8585,  0.9786,  0.9940, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9998, -0.9998, -0.9984,  0.9999, -0.9985]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9419, -1.0000, -0.9561,  0.9875]]) tensor([ 4])\n",
      "epoch 57, loss 4.208147048950195\n",
      "tensor([[-0.8319,  0.8496, -0.5345, -0.9448, -0.9428]]) tensor([ 1])\n",
      "tensor([[ 0.9916,  0.9975, -0.5183, -1.0000, -0.9388]]) tensor([ 0])\n",
      "tensor([[ 0.9284, -0.8407,  0.9785, -0.9979, -0.9968]]) tensor([ 2])\n",
      "tensor([[-0.3188, -0.8465,  0.9771,  0.9940, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9996, -0.9989,  0.9999, -0.9976]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9391, -1.0000, -0.9584,  0.9880]]) tensor([ 4])\n",
      "epoch 58, loss 4.176071643829346\n",
      "tensor([[-0.8429,  0.8612, -0.5607, -0.9445, -0.9453]]) tensor([ 1])\n",
      "tensor([[ 0.9919,  0.9978, -0.5742, -1.0000, -0.9387]]) tensor([ 0])\n",
      "tensor([[ 0.9211, -0.8365,  0.9745, -0.9985, -0.9963]]) tensor([ 2])\n",
      "tensor([[-0.4428, -0.8339,  0.9754,  0.9940, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9992, -0.9992,  0.9999, -0.9964]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9363, -1.0000, -0.9606,  0.9884]]) tensor([ 4])\n",
      "epoch 59, loss 4.148719310760498\n",
      "tensor([[-0.8530,  0.8713, -0.5850, -0.9443, -0.9477]]) tensor([ 1])\n",
      "tensor([[ 0.9921,  0.9980, -0.6239, -1.0000, -0.9396]]) tensor([ 0])\n",
      "tensor([[ 0.9132, -0.8339,  0.9701, -0.9988, -0.9958]]) tensor([ 2])\n",
      "tensor([[-0.5513, -0.8204,  0.9735,  0.9940, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9985, -0.9994,  0.9998, -0.9949]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9336, -1.0000, -0.9626,  0.9888]]) tensor([ 4])\n",
      "epoch 60, loss 4.125778675079346\n",
      "tensor([[-0.8623,  0.8802, -0.6072, -0.9441, -0.9500]]) tensor([ 1])\n",
      "tensor([[ 0.9923,  0.9982, -0.6671, -1.0000, -0.9414]]) tensor([ 0])\n",
      "tensor([[ 0.9047, -0.8330,  0.9656, -0.9991, -0.9955]]) tensor([ 2])\n",
      "tensor([[-0.6423, -0.8060,  0.9713,  0.9939, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9973, -0.9995,  0.9997, -0.9930]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9311, -1.0000, -0.9646,  0.9891]]) tensor([ 4])\n",
      "epoch 61, loss 4.106576919555664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8709,  0.8880, -0.6276, -0.9441, -0.9523]]) tensor([ 1])\n",
      "tensor([[ 0.9925,  0.9984, -0.7040, -1.0000, -0.9440]]) tensor([ 0])\n",
      "tensor([[ 0.8953, -0.8335,  0.9612, -0.9993, -0.9953]]) tensor([ 2])\n",
      "tensor([[-0.7163, -0.7902,  0.9688,  0.9938, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9957, -0.9996,  0.9997, -0.9909]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9287, -1.0000, -0.9664,  0.9895]]) tensor([ 4])\n",
      "epoch 62, loss 4.090289115905762\n",
      "tensor([[-0.8786,  0.8947, -0.6460, -0.9441, -0.9544]]) tensor([ 1])\n",
      "tensor([[ 0.9926,  0.9985, -0.7349, -1.0000, -0.9472]]) tensor([ 0])\n",
      "tensor([[ 0.8847, -0.8353,  0.9571, -0.9994, -0.9952]]) tensor([ 2])\n",
      "tensor([[-0.7751, -0.7727,  0.9660,  0.9937, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9936, -0.9997,  0.9996, -0.9886]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9265, -1.0000, -0.9681,  0.9898]]) tensor([ 4])\n",
      "epoch 63, loss 4.076082229614258\n",
      "tensor([[-0.8857,  0.9006, -0.6626, -0.9441, -0.9565]]) tensor([ 1])\n",
      "tensor([[ 0.9927,  0.9986, -0.7606, -1.0000, -0.9508]]) tensor([ 0])\n",
      "tensor([[ 0.8725, -0.8381,  0.9535, -0.9995, -0.9953]]) tensor([ 2])\n",
      "tensor([[-0.8212, -0.7530,  0.9628,  0.9936, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9913, -0.9997,  0.9995, -0.9862]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9246, -1.0000, -0.9697,  0.9901]]) tensor([ 4])\n",
      "epoch 64, loss 4.063201427459717\n",
      "tensor([[-0.8922,  0.9057, -0.6775, -0.9442, -0.9584]]) tensor([ 1])\n",
      "tensor([[ 0.9927,  0.9987, -0.7817, -1.0000, -0.9547]]) tensor([ 0])\n",
      "tensor([[ 0.8582, -0.8418,  0.9505, -0.9995, -0.9955]]) tensor([ 2])\n",
      "tensor([[-0.8571, -0.7301,  0.9591,  0.9935, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9889, -0.9997,  0.9994, -0.9838]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9229, -1.0000, -0.9713,  0.9903]]) tensor([ 4])\n",
      "epoch 65, loss 4.050997734069824\n",
      "tensor([[-0.8981,  0.9101, -0.6910, -0.9444, -0.9603]]) tensor([ 1])\n",
      "tensor([[ 0.9926,  0.9987, -0.7989, -1.0000, -0.9587]]) tensor([ 0])\n",
      "tensor([[ 0.8412, -0.8460,  0.9483, -0.9996, -0.9957]]) tensor([ 2])\n",
      "tensor([[-0.8850, -0.7029,  0.9548,  0.9933, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9865, -0.9998,  0.9993, -0.9813]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9214, -1.0000, -0.9727,  0.9905]]) tensor([ 4])\n",
      "epoch 66, loss 4.0389404296875\n",
      "tensor([[-0.9036,  0.9138, -0.7031, -0.9446, -0.9621]]) tensor([ 1])\n",
      "tensor([[ 0.9924,  0.9988, -0.8128, -1.0000, -0.9625]]) tensor([ 0])\n",
      "tensor([[ 0.8209, -0.8508,  0.9470, -0.9996, -0.9960]]) tensor([ 2])\n",
      "tensor([[-0.9067, -0.6698,  0.9495,  0.9931, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9843, -0.9998,  0.9991, -0.9785]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9202, -1.0000, -0.9740,  0.9907]]) tensor([ 4])\n",
      "epoch 67, loss 4.026614189147949\n",
      "tensor([[-0.9087,  0.9169, -0.7141, -0.9449, -0.9637]]) tensor([ 1])\n",
      "tensor([[ 0.9922,  0.9989, -0.8239, -1.0000, -0.9663]]) tensor([ 0])\n",
      "tensor([[ 0.7965, -0.8560,  0.9464, -0.9997, -0.9963]]) tensor([ 2])\n",
      "tensor([[-0.9238, -0.6284,  0.9431,  0.9928, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9824, -0.9998,  0.9990, -0.9752]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9192, -1.0000, -0.9752,  0.9909]]) tensor([ 4])\n",
      "epoch 68, loss 4.0137248039245605\n",
      "tensor([[-0.9134,  0.9193, -0.7242, -0.9451, -0.9653]]) tensor([ 1])\n",
      "tensor([[ 0.9919,  0.9989, -0.8328, -1.0000, -0.9697]]) tensor([ 0])\n",
      "tensor([[ 0.7671, -0.8615,  0.9465, -0.9997, -0.9967]]) tensor([ 2])\n",
      "tensor([[-0.9373, -0.5758,  0.9348,  0.9924, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9808, -0.9998,  0.9987, -0.9712]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9183, -1.0000, -0.9763,  0.9911]]) tensor([ 4])\n",
      "epoch 69, loss 4.000127792358398\n",
      "tensor([[-0.9178,  0.9212, -0.7334, -0.9454, -0.9668]]) tensor([ 1])\n",
      "tensor([[ 0.9914,  0.9990, -0.8399, -1.0000, -0.9729]]) tensor([ 0])\n",
      "tensor([[ 0.7318, -0.8674,  0.9473, -0.9997, -0.9970]]) tensor([ 2])\n",
      "tensor([[-0.9480, -0.5085,  0.9240,  0.9919, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9797, -0.9998,  0.9984, -0.9658]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9175, -1.0000, -0.9773,  0.9912]]) tensor([ 4])\n",
      "epoch 70, loss 3.985882043838501\n",
      "tensor([[-0.9218,  0.9223, -0.7421, -0.9457, -0.9682]]) tensor([ 1])\n",
      "tensor([[ 0.9909,  0.9990, -0.8454, -1.0000, -0.9757]]) tensor([ 0])\n",
      "tensor([[ 0.6902, -0.8737,  0.9485, -0.9997, -0.9972]]) tensor([ 2])\n",
      "tensor([[-0.9566, -0.4231,  0.9095,  0.9911, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9791, -0.9998,  0.9978, -0.9581]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9164, -1.0000, -0.9783,  0.9914]]) tensor([ 4])\n",
      "epoch 71, loss 3.971325159072876\n",
      "tensor([[-0.9255,  0.9228, -0.7503, -0.9460, -0.9695]]) tensor([ 1])\n",
      "tensor([[ 0.9903,  0.9990, -0.8497, -1.0000, -0.9782]]) tensor([ 0])\n",
      "tensor([[ 0.6427, -0.8804,  0.9500, -0.9997, -0.9975]]) tensor([ 2])\n",
      "tensor([[-0.9636, -0.3194,  0.8898,  0.9902, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9790, -0.9998,  0.9969, -0.9466]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9149, -1.0000, -0.9793,  0.9916]]) tensor([ 4])\n",
      "epoch 72, loss 3.957070827484131\n",
      "tensor([[-0.9289,  0.9223, -0.7582, -0.9463, -0.9708]]) tensor([ 1])\n",
      "tensor([[ 0.9896,  0.9990, -0.8529, -1.0000, -0.9804]]) tensor([ 0])\n",
      "tensor([[ 0.5913, -0.8876,  0.9518, -0.9997, -0.9977]]) tensor([ 2])\n",
      "tensor([[-0.9692, -0.2043,  0.8633,  0.9890, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9998, -0.9792, -0.9998,  0.9954, -0.9297]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9126, -1.0000, -0.9803,  0.9920]]) tensor([ 4])\n",
      "epoch 73, loss 3.943709135055542\n",
      "tensor([[-0.9319,  0.9208, -0.7661, -0.9466, -0.9719]]) tensor([ 1])\n",
      "tensor([[ 0.9890,  0.9991, -0.8553, -1.0000, -0.9823]]) tensor([ 0])\n",
      "tensor([[ 0.5405, -0.8954,  0.9536, -0.9997, -0.9979]]) tensor([ 2])\n",
      "tensor([[-0.9737, -0.0957,  0.8291,  0.9878, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9998, -0.9797, -0.9998,  0.9928, -0.9063]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9091, -1.0000, -0.9813,  0.9924]]) tensor([ 4])\n",
      "epoch 74, loss 3.931027889251709\n",
      "tensor([[-0.9343,  0.9181, -0.7739, -0.9468, -0.9731]]) tensor([ 1])\n",
      "tensor([[ 0.9885,  0.9991, -0.8569, -1.0000, -0.9840]]) tensor([ 0])\n",
      "tensor([[ 0.4970, -0.9038,  0.9556, -0.9997, -0.9981]]) tensor([ 2])\n",
      "tensor([[-0.9772, -0.0202,  0.7879,  0.9868, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9997, -0.9801, -0.9998,  0.9890, -0.8790]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9046, -1.0000, -0.9823,  0.9928]]) tensor([ 4])\n",
      "epoch 75, loss 3.91723370552063\n",
      "tensor([[-0.9362,  0.9140, -0.7817, -0.9469, -0.9742]]) tensor([ 1])\n",
      "tensor([[ 0.9882,  0.9991, -0.8580, -1.0000, -0.9855]]) tensor([ 0])\n",
      "tensor([[ 0.4681, -0.9126,  0.9575, -0.9997, -0.9983]]) tensor([ 2])\n",
      "tensor([[-0.9799, -0.0038,  0.7434,  0.9864, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9997, -0.9803, -0.9999,  0.9848, -0.8561]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9006, -1.0000, -0.9832,  0.9930]]) tensor([ 4])\n",
      "epoch 76, loss 3.8992390632629395\n",
      "tensor([[-0.9376,  0.9083, -0.7895, -0.9469, -0.9752]]) tensor([ 1])\n",
      "tensor([[ 0.9883,  0.9991, -0.8585, -1.0000, -0.9868]]) tensor([ 0])\n",
      "tensor([[ 0.4581, -0.9216,  0.9595, -0.9997, -0.9984]]) tensor([ 2])\n",
      "tensor([[-0.9820, -0.0617,  0.7008,  0.9869, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9998, -0.9802, -0.9999,  0.9822, -0.8482]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8998, -1.0000, -0.9836,  0.9928]]) tensor([ 4])\n",
      "epoch 77, loss 3.8747291564941406\n",
      "tensor([[-0.9385,  0.9011, -0.7973, -0.9469, -0.9763]]) tensor([ 1])\n",
      "tensor([[ 0.9887,  0.9991, -0.8586, -1.0000, -0.9879]]) tensor([ 0])\n",
      "tensor([[ 0.4653, -0.9303,  0.9613, -0.9997, -0.9985]]) tensor([ 2])\n",
      "tensor([[-0.9836, -0.1851,  0.6620,  0.9881, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9998, -0.9801, -0.9999,  0.9822, -0.8570]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9026, -1.0000, -0.9836,  0.9922]]) tensor([ 4])\n",
      "epoch 78, loss 3.845496654510498\n",
      "tensor([[-0.9391,  0.8924, -0.8053, -0.9467, -0.9773]]) tensor([ 1])\n",
      "tensor([[ 0.9892,  0.9992, -0.8587, -1.0000, -0.9890]]) tensor([ 0])\n",
      "tensor([[ 0.4806, -0.9384,  0.9631, -0.9997, -0.9986]]) tensor([ 2])\n",
      "tensor([[-0.9849, -0.3347,  0.6219,  0.9895, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9800, -0.9999,  0.9832, -0.8699]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9064, -1.0000, -0.9834,  0.9913]]) tensor([ 4])\n",
      "epoch 79, loss 3.816749334335327\n",
      "tensor([[-0.9398,  0.8828, -0.8134, -0.9465, -0.9783]]) tensor([ 1])\n",
      "tensor([[ 0.9895,  0.9992, -0.8587, -1.0000, -0.9899]]) tensor([ 0])\n",
      "tensor([[ 0.4928, -0.9455,  0.9645, -0.9997, -0.9987]]) tensor([ 2])\n",
      "tensor([[-0.9860, -0.4654,  0.5704,  0.9906, -1.0000]]) tensor([ 3])\n",
      "tensor([[-0.9999, -0.9800, -1.0000,  0.9823, -0.8724]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9081, -1.0000, -0.9834,  0.9905]]) tensor([ 4])\n",
      "epoch 80, loss 3.7906556129455566\n",
      "tensor([[-0.9405,  0.8726, -0.8216, -0.9463, -0.9793]]) tensor([ 1])\n",
      "tensor([[ 0.9897,  0.9992, -0.8587, -1.0000, -0.9908]]) tensor([ 0])\n",
      "tensor([[ 0.4948, -0.9514,  0.9657, -0.9997, -0.9988]]) tensor([ 2])\n",
      "tensor([[-0.9870, -0.5560,  0.4977,  0.9913, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9800, -1.0000,  0.9768, -0.8547]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9055, -1.0000, -0.9837,  0.9901]]) tensor([ 4])\n",
      "epoch 81, loss 3.7656784057617188\n",
      "tensor([[-0.9415,  0.8623, -0.8299, -0.9460, -0.9803]]) tensor([ 1])\n",
      "tensor([[ 0.9896,  0.9992, -0.8587, -1.0000, -0.9916]]) tensor([ 0])\n",
      "tensor([[ 0.4855, -0.9564,  0.9668, -0.9997, -0.9988]]) tensor([ 2])\n",
      "tensor([[-0.9879, -0.6110,  0.3996,  0.9916, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9798, -1.0000,  0.9620, -0.8066]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8949, -1.0000, -0.9847,  0.9903]]) tensor([ 4])\n",
      "epoch 82, loss 3.7430737018585205\n",
      "tensor([[-0.9424,  0.8521, -0.8376, -0.9456, -0.9812]]) tensor([ 1])\n",
      "tensor([[ 0.9894,  0.9992, -0.8587, -1.0000, -0.9923]]) tensor([ 0])\n",
      "tensor([[ 0.4734, -0.9604,  0.9678, -0.9998, -0.9989]]) tensor([ 2])\n",
      "tensor([[-0.9887, -0.6535,  0.2910,  0.9919, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9794, -1.0000,  0.9324, -0.7323]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8738, -1.0000, -0.9863,  0.9908]]) tensor([ 4])\n",
      "epoch 83, loss 3.730163812637329\n",
      "tensor([[-0.9429,  0.8421, -0.8437, -0.9450, -0.9822]]) tensor([ 1])\n",
      "tensor([[ 0.9896,  0.9992, -0.8590, -1.0000, -0.9931]]) tensor([ 0])\n",
      "tensor([[ 0.4794, -0.9638,  0.9691, -0.9998, -0.9990]]) tensor([ 2])\n",
      "tensor([[-0.9893, -0.7208,  0.2194,  0.9927, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9795, -1.0000,  0.9124, -0.7074]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8651, -1.0000, -0.9865,  0.9896]]) tensor([ 4])\n",
      "epoch 84, loss 3.721553087234497\n",
      "tensor([[-0.9429,  0.8329, -0.8477, -0.9440, -0.9833]]) tensor([ 1])\n",
      "tensor([[ 0.9902,  0.9993, -0.8597, -1.0000, -0.9939]]) tensor([ 0])\n",
      "tensor([[ 0.5091, -0.9664,  0.9710, -0.9998, -0.9990]]) tensor([ 2])\n",
      "tensor([[-0.9897, -0.8046,  0.2089,  0.9940, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9806, -1.0000,  0.9282, -0.7718]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8869, -1.0000, -0.9839,  0.9850]]) tensor([ 4])\n",
      "epoch 85, loss 3.707573890686035\n",
      "tensor([[-0.9429,  0.8255, -0.8508, -0.9427, -0.9843]]) tensor([ 1])\n",
      "tensor([[ 0.9907,  0.9993, -0.8607, -1.0000, -0.9946]]) tensor([ 0])\n",
      "tensor([[ 0.5366, -0.9683,  0.9730, -0.9998, -0.9990]]) tensor([ 2])\n",
      "tensor([[-0.9900, -0.8613,  0.2123,  0.9951, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9820, -1.0000,  0.9445, -0.8325]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9050, -1.0000, -0.9804,  0.9780]]) tensor([ 4])\n",
      "epoch 86, loss 3.700822114944458\n",
      "tensor([[-0.9432,  0.8205, -0.8536, -0.9413, -0.9852]]) tensor([ 1])\n",
      "tensor([[ 0.9910,  0.9993, -0.8616, -1.0000, -0.9951]]) tensor([ 0])\n",
      "tensor([[ 0.5465, -0.9696,  0.9747, -0.9998, -0.9990]]) tensor([ 2])\n",
      "tensor([[-0.9903, -0.8884,  0.1945,  0.9958, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9830, -1.0000,  0.9485, -0.8588]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9126, -1.0000, -0.9772,  0.9705]]) tensor([ 4])\n",
      "epoch 87, loss 3.695491313934326\n",
      "tensor([[-0.9439,  0.8182, -0.8566, -0.9399, -0.9860]]) tensor([ 1])\n",
      "tensor([[ 0.9911,  0.9993, -0.8621, -1.0000, -0.9956]]) tensor([ 0])\n",
      "tensor([[ 0.5358, -0.9702,  0.9763, -0.9998, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9907, -0.8954,  0.1461,  0.9961, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9833, -1.0000,  0.9395, -0.8549]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9123, -1.0000, -0.9747,  0.9643]]) tensor([ 4])\n",
      "epoch 88, loss 3.686009645462036\n",
      "tensor([[-0.9449,  0.8185, -0.8594, -0.9383, -0.9867]]) tensor([ 1])\n",
      "tensor([[ 0.9909,  0.9993, -0.8625, -1.0000, -0.9959]]) tensor([ 0])\n",
      "tensor([[ 0.5089, -0.9702,  0.9777, -0.9998, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9911, -0.8888,  0.0757,  0.9962, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9830, -1.0000,  0.9165, -0.8262]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9049, -1.0000, -0.9732,  0.9601]]) tensor([ 4])\n",
      "epoch 89, loss 3.675400733947754\n",
      "tensor([[-0.9459,  0.8211, -0.8614, -0.9364, -0.9873]]) tensor([ 1])\n",
      "tensor([[ 0.9907,  0.9994, -0.8631, -1.0000, -0.9962]]) tensor([ 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4788, -0.9698,  0.9791, -0.9998, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9914, -0.8769,  0.0143,  0.9963, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9828, -1.0000,  0.8924, -0.7955]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8961, -1.0000, -0.9710,  0.9570]]) tensor([ 4])\n",
      "epoch 90, loss 3.667187452316284\n",
      "tensor([[-0.9466,  0.8254, -0.8620, -0.9340, -0.9878]]) tensor([ 1])\n",
      "tensor([[ 0.9909,  0.9994, -0.8644, -1.0000, -0.9964]]) tensor([ 0])\n",
      "tensor([[ 0.4628, -0.9690,  0.9806, -0.9998, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9917, -0.8735,  0.0004,  0.9967, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9834, -1.0000,  0.9028, -0.8081]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9021, -1.0000, -0.9637,  0.9510]]) tensor([ 4])\n",
      "epoch 91, loss 3.653881311416626\n",
      "tensor([[-0.9472,  0.8311, -0.8617, -0.9313, -0.9882]]) tensor([ 1])\n",
      "tensor([[ 0.9911,  0.9994, -0.8661, -1.0000, -0.9966]]) tensor([ 0])\n",
      "tensor([[ 0.4549, -0.9678,  0.9822, -0.9998, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9919, -0.8750,  0.0170,  0.9971, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9845, -1.0000,  0.9305, -0.8427]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9145, -1.0000, -0.9505,  0.9434]]) tensor([ 4])\n",
      "epoch 92, loss 3.6399664878845215\n",
      "tensor([[-0.9478,  0.8380, -0.8614, -0.9285, -0.9885]]) tensor([ 1])\n",
      "tensor([[ 0.9912,  0.9994, -0.8679, -1.0000, -0.9967]]) tensor([ 0])\n",
      "tensor([[ 0.4360, -0.9662,  0.9836, -0.9998, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9921, -0.8659,  0.0178,  0.9973, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9852, -1.0000,  0.9439, -0.8530]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9193, -1.0000, -0.9358,  0.9408]]) tensor([ 4])\n",
      "epoch 93, loss 3.6276659965515137\n",
      "tensor([[-0.9488,  0.8456, -0.8615, -0.9256, -0.9887]]) tensor([ 1])\n",
      "tensor([[ 0.9911,  0.9995, -0.8698, -1.0000, -0.9967]]) tensor([ 0])\n",
      "tensor([[ 0.3988, -0.9642,  0.9847, -0.9998, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9923, -0.8367, -0.0170,  0.9974, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9852, -1.0000,  0.9393, -0.8236]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9141, -1.0000, -0.9239,  0.9470]]) tensor([ 4])\n",
      "epoch 94, loss 3.6131844520568848\n",
      "tensor([[-0.9499,  0.8533, -0.8616, -0.9227, -0.9889]]) tensor([ 1])\n",
      "tensor([[ 0.9909,  0.9995, -0.8719, -1.0000, -0.9967]]) tensor([ 0])\n",
      "tensor([[ 0.3531, -0.9619,  0.9857, -0.9999, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9925, -0.7901, -0.0673,  0.9974, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9848, -1.0000,  0.9232, -0.7606]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8994, -1.0000, -0.9158,  0.9585]]) tensor([ 4])\n",
      "epoch 95, loss 3.602612257003784\n",
      "tensor([[-0.9505,  0.8606, -0.8611, -0.9198, -0.9890]]) tensor([ 1])\n",
      "tensor([[ 0.9910,  0.9995, -0.8744, -1.0000, -0.9967]]) tensor([ 0])\n",
      "tensor([[ 0.3294, -0.9594,  0.9867, -0.9999, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9926, -0.7653, -0.0713,  0.9976, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9853, -1.0000,  0.9323, -0.7493]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8992, -1.0000, -0.8955,  0.9628]]) tensor([ 4])\n",
      "epoch 96, loss 3.5926618576049805\n",
      "tensor([[-0.9509,  0.8673, -0.8602, -0.9168, -0.9891]]) tensor([ 1])\n",
      "tensor([[ 0.9913,  0.9995, -0.8772, -1.0000, -0.9967]]) tensor([ 0])\n",
      "tensor([[ 0.3264, -0.9568,  0.9876, -0.9999, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9927, -0.7696, -0.0338,  0.9979, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9866, -1.0000,  0.9582, -0.7955]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9142, -1.0000, -0.8566,  0.9597]]) tensor([ 4])\n",
      "epoch 97, loss 3.584240674972534\n",
      "tensor([[-0.9514,  0.8734, -0.8597, -0.9142, -0.9893]]) tensor([ 1])\n",
      "tensor([[ 0.9915,  0.9995, -0.8801, -1.0000, -0.9966]]) tensor([ 0])\n",
      "tensor([[ 0.3112, -0.9540,  0.9884, -0.9999, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9928, -0.7568, -0.0251,  0.9981, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9873, -1.0000,  0.9665, -0.8011]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9175, -1.0000, -0.8238,  0.9615]]) tensor([ 4])\n",
      "epoch 98, loss 3.579042434692383\n",
      "tensor([[-0.9521,  0.8791, -0.8600, -0.9121, -0.9893]]) tensor([ 1])\n",
      "tensor([[ 0.9914,  0.9996, -0.8829, -1.0000, -0.9966]]) tensor([ 0])\n",
      "tensor([[ 0.2773, -0.9511,  0.9890, -0.9999, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9929, -0.7134, -0.0610,  0.9981, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9871, -1.0000,  0.9591, -0.7456]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9059, -1.0000, -0.8118,  0.9704]]) tensor([ 4])\n",
      "epoch 99, loss 3.572998285293579\n",
      "tensor([[-0.9528,  0.8839, -0.8605, -0.9104, -0.9894]]) tensor([ 1])\n",
      "tensor([[ 0.9914,  0.9996, -0.8857, -1.0000, -0.9966]]) tensor([ 0])\n",
      "tensor([[ 0.2496, -0.9483,  0.9895, -0.9999, -0.9991]]) tensor([ 2])\n",
      "tensor([[-0.9930, -0.6776, -0.0898,  0.9981, -1.0000]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.9871, -1.0000,  0.9522, -0.6932]]) tensor([ 3])\n",
      "tensor([[-1.0000, -0.8937, -1.0000, -0.8031,  0.9768]]) tensor([ 4])\n",
      "epoch 100, loss 3.5704703330993652\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "criterion = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(model.parameters(), lr = 0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "    for input, label in zip(inputs, labels):\n",
    "        hidden, output = model(input, hidden)\n",
    "        val, idx = output.max(1)\n",
    "        #print(idx2char[idx.data[0]])\n",
    "        #print(output, label.view(-1))\n",
    "        loss += criterion(output, label.view(-1))\n",
    "    print(f'epoch {epoch + 1}, loss {loss.data[0]}')\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_s = 1\n",
    "seq_l = 6\n",
    "class Model2(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.rnn = t.nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        hidden = self.init_hidden()\n",
    "        x = x.view(batch_s, seq_l, input_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        return out.view(-1, num_classes)\n",
    "    def init_hidden(self):\n",
    "        return t.zeros(num_layers, batch_s, hidden_size) # hidden is 1 per hidden layer? or implicit cell input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.6415597200393677\n",
      "Predicted string:  eeeeee\n",
      "epoch 2, loss 1.4516552686691284\n",
      "Predicted string:  eellll\n",
      "epoch 3, loss 1.3398113250732422\n",
      "Predicted string:  lhllll\n",
      "epoch 4, loss 1.2325706481933594\n",
      "Predicted string:  lhllll\n",
      "epoch 5, loss 1.1222553253173828\n",
      "Predicted string:  lhelll\n",
      "epoch 6, loss 1.0544041395187378\n",
      "Predicted string:  lhelll\n",
      "epoch 7, loss 0.9917203783988953\n",
      "Predicted string:  lhelll\n",
      "epoch 8, loss 0.929325520992279\n",
      "Predicted string:  ihelll\n",
      "epoch 9, loss 0.87046879529953\n",
      "Predicted string:  ihelll\n",
      "epoch 10, loss 0.8156734108924866\n",
      "Predicted string:  ihello\n",
      "epoch 11, loss 0.7674911618232727\n",
      "Predicted string:  ihello\n",
      "epoch 12, loss 0.7274844646453857\n",
      "Predicted string:  ihello\n",
      "epoch 13, loss 0.6987817287445068\n",
      "Predicted string:  ihello\n",
      "epoch 14, loss 0.6784595847129822\n",
      "Predicted string:  ihello\n",
      "epoch 15, loss 0.6577062606811523\n",
      "Predicted string:  ihello\n",
      "epoch 16, loss 0.6338006854057312\n",
      "Predicted string:  ihello\n",
      "epoch 17, loss 0.6107373833656311\n",
      "Predicted string:  ihello\n",
      "epoch 18, loss 0.5920802354812622\n",
      "Predicted string:  ihello\n",
      "epoch 19, loss 0.5804914236068726\n",
      "Predicted string:  ihello\n",
      "epoch 20, loss 0.5707486271858215\n",
      "Predicted string:  ihello\n",
      "epoch 21, loss 0.5560839176177979\n",
      "Predicted string:  ihello\n",
      "epoch 22, loss 0.5433683395385742\n",
      "Predicted string:  ihello\n",
      "epoch 23, loss 0.534757673740387\n",
      "Predicted string:  ihello\n",
      "epoch 24, loss 0.5287976861000061\n",
      "Predicted string:  ihello\n",
      "epoch 25, loss 0.5228026509284973\n",
      "Predicted string:  ihello\n",
      "epoch 26, loss 0.5156764984130859\n",
      "Predicted string:  ihello\n",
      "epoch 27, loss 0.5090770125389099\n",
      "Predicted string:  ihello\n",
      "epoch 28, loss 0.5054246187210083\n",
      "Predicted string:  ihello\n",
      "epoch 29, loss 0.5033048987388611\n",
      "Predicted string:  ihello\n",
      "epoch 30, loss 0.49915945529937744\n",
      "Predicted string:  ihello\n",
      "epoch 31, loss 0.495038777589798\n",
      "Predicted string:  ihello\n",
      "epoch 32, loss 0.492898553609848\n",
      "Predicted string:  ihello\n",
      "epoch 33, loss 0.49131646752357483\n",
      "Predicted string:  ihello\n",
      "epoch 34, loss 0.4889293909072876\n",
      "Predicted string:  ihello\n",
      "epoch 35, loss 0.48585936427116394\n",
      "Predicted string:  ihello\n",
      "epoch 36, loss 0.4834030866622925\n",
      "Predicted string:  ihello\n",
      "epoch 37, loss 0.4823285639286041\n",
      "Predicted string:  ihello\n",
      "epoch 38, loss 0.480817586183548\n",
      "Predicted string:  ihello\n",
      "epoch 39, loss 0.4784359931945801\n",
      "Predicted string:  ihello\n",
      "epoch 40, loss 0.4771171510219574\n",
      "Predicted string:  ihello\n",
      "epoch 41, loss 0.4764661490917206\n",
      "Predicted string:  ihello\n",
      "epoch 42, loss 0.475353479385376\n",
      "Predicted string:  ihello\n",
      "epoch 43, loss 0.4739220440387726\n",
      "Predicted string:  ihello\n",
      "epoch 44, loss 0.4731246531009674\n",
      "Predicted string:  ihello\n",
      "epoch 45, loss 0.47269561886787415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achang/miniconda2/envs/deep-learning-lab/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string:  ihello\n",
      "epoch 46, loss 0.471571683883667\n",
      "Predicted string:  ihello\n",
      "epoch 47, loss 0.4706486165523529\n",
      "Predicted string:  ihello\n",
      "epoch 48, loss 0.47025489807128906\n",
      "Predicted string:  ihello\n",
      "epoch 49, loss 0.4696974754333496\n",
      "Predicted string:  ihello\n",
      "epoch 50, loss 0.4689004719257355\n",
      "Predicted string:  ihello\n",
      "epoch 51, loss 0.4684046804904938\n",
      "Predicted string:  ihello\n",
      "epoch 52, loss 0.46818193793296814\n",
      "Predicted string:  ihello\n",
      "epoch 53, loss 0.4676450788974762\n",
      "Predicted string:  ihello\n",
      "epoch 54, loss 0.4671362340450287\n",
      "Predicted string:  ihello\n",
      "epoch 55, loss 0.4668966233730316\n",
      "Predicted string:  ihello\n",
      "epoch 56, loss 0.46656182408332825\n",
      "Predicted string:  ihello\n",
      "epoch 57, loss 0.4660802185535431\n",
      "Predicted string:  ihello\n",
      "epoch 58, loss 0.4657630920410156\n",
      "Predicted string:  ihello\n",
      "epoch 59, loss 0.46556004881858826\n",
      "Predicted string:  ihello\n",
      "epoch 60, loss 0.46519505977630615\n",
      "Predicted string:  ihello\n",
      "epoch 61, loss 0.46489599347114563\n",
      "Predicted string:  ihello\n",
      "epoch 62, loss 0.46473249793052673\n",
      "Predicted string:  ihello\n",
      "epoch 63, loss 0.46448978781700134\n",
      "Predicted string:  ihello\n",
      "epoch 64, loss 0.4642009437084198\n",
      "Predicted string:  ihello\n",
      "epoch 65, loss 0.46401914954185486\n",
      "Predicted string:  ihello\n",
      "epoch 66, loss 0.4638245105743408\n",
      "Predicted string:  ihello\n",
      "epoch 67, loss 0.4635562002658844\n",
      "Predicted string:  ihello\n",
      "epoch 68, loss 0.463364839553833\n",
      "Predicted string:  ihello\n",
      "epoch 69, loss 0.463199257850647\n",
      "Predicted string:  ihello\n",
      "epoch 70, loss 0.4629794657230377\n",
      "Predicted string:  ihello\n",
      "epoch 71, loss 0.46278953552246094\n",
      "Predicted string:  ihello\n",
      "epoch 72, loss 0.4626440703868866\n",
      "Predicted string:  ihello\n",
      "epoch 73, loss 0.4624522626399994\n",
      "Predicted string:  ihello\n",
      "epoch 74, loss 0.4622671902179718\n",
      "Predicted string:  ihello\n",
      "epoch 75, loss 0.46212077140808105\n",
      "Predicted string:  ihello\n",
      "epoch 76, loss 0.4619487226009369\n",
      "Predicted string:  ihello\n",
      "epoch 77, loss 0.4617718756198883\n",
      "Predicted string:  ihello\n",
      "epoch 78, loss 0.461631178855896\n",
      "Predicted string:  ihello\n",
      "epoch 79, loss 0.4614778459072113\n",
      "Predicted string:  ihello\n",
      "epoch 80, loss 0.4613170325756073\n",
      "Predicted string:  ihello\n",
      "epoch 81, loss 0.46118342876434326\n",
      "Predicted string:  ihello\n",
      "epoch 82, loss 0.46104201674461365\n",
      "Predicted string:  ihello\n",
      "epoch 83, loss 0.46089205145835876\n",
      "Predicted string:  ihello\n",
      "epoch 84, loss 0.4607623517513275\n",
      "Predicted string:  ihello\n",
      "epoch 85, loss 0.4606282711029053\n",
      "Predicted string:  ihello\n",
      "epoch 86, loss 0.46048805117607117\n",
      "Predicted string:  ihello\n",
      "epoch 87, loss 0.46036407351493835\n",
      "Predicted string:  ihello\n",
      "epoch 88, loss 0.460237056016922\n",
      "Predicted string:  ihello\n",
      "epoch 89, loss 0.4601064920425415\n",
      "Predicted string:  ihello\n",
      "epoch 90, loss 0.45998823642730713\n",
      "Predicted string:  ihello\n",
      "epoch 91, loss 0.45986685156822205\n",
      "Predicted string:  ihello\n",
      "epoch 92, loss 0.4597437083721161\n",
      "Predicted string:  ihello\n",
      "epoch 93, loss 0.4596295654773712\n",
      "Predicted string:  ihello\n",
      "epoch 94, loss 0.45951223373413086\n",
      "Predicted string:  ihello\n",
      "epoch 95, loss 0.4593949317932129\n",
      "Predicted string:  ihello\n",
      "epoch 96, loss 0.45928463339805603\n",
      "Predicted string:  ihello\n",
      "epoch 97, loss 0.459171324968338\n",
      "Predicted string:  ihello\n",
      "epoch 98, loss 0.4590599536895752\n",
      "Predicted string:  ihello\n",
      "epoch 99, loss 0.4589533507823944\n",
      "Predicted string:  ihello\n",
      "epoch 100, loss 0.458844393491745\n",
      "Predicted string:  ihello\n"
     ]
    }
   ],
   "source": [
    "model = Model2()\n",
    "criterion = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(model.parameters(), lr = 0.1)\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    outputs = model(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(f'epoch {epoch + 1}, loss {loss.data[0]}')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    #print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data[0]))\n",
    "    print(\"Predicted string: \", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12-2\n",
    "Softmax as an output is more stables because forces numbers between 0 / 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding idea\n",
    "\n",
    "Use a lookup table versus one hot ... like a combination of features to represent something versus one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = t.nn.Embedding(5, 10) # 5 parts to a sentence, now it's represented by 10 pieces uniquely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4162,  1.1143,  1.1127,  1.6606,  0.3824, -0.1259, -1.1290,\n",
       "          1.3560,  0.1015, -1.5492],\n",
       "        [-0.1946, -0.3707,  1.2166, -0.5027,  1.8302, -0.3568,  0.3462,\n",
       "         -1.5733, -1.5489, -0.3795],\n",
       "        [-0.4162,  1.1143,  1.1127,  1.6606,  0.3824, -0.1259, -1.1290,\n",
       "          1.3560,  0.1015, -1.5492],\n",
       "        [ 0.5127, -0.5082, -0.0649, -0.5959,  1.1138,  0.2975,  0.8366,\n",
       "          0.2352, -0.5933, -0.6783],\n",
       "        [ 0.7313,  0.3983,  2.1342, -0.0237,  0.2575, -1.1401,  1.2834,\n",
       "          0.0958, -1.0707, -1.0808],\n",
       "        [ 0.7313,  0.3983,  2.1342, -0.0237,  0.2575, -1.1401,  1.2834,\n",
       "          0.0958, -1.0707, -1.0808]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(t.LongTensor(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12-3 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.varunajayasiri.com/numpy_lstm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b, c$ are bias terms\n",
    "\n",
    "$h^{t-1}$ is hidden from before, with $w$ as its weight\n",
    "\n",
    "$U$ is weight for input $x^t$\n",
    "\n",
    "$V$ is weight for $h^t$\n",
    "\n",
    "$y^t = softmax(o^t)$ output\n",
    "\n",
    "$o^t = c + Vh^t$ \n",
    "\n",
    "$h^t = tanh(a^t)$\n",
    "\n",
    "$a^t = b + w^{t-1} + Ux^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs224d.stanford.edu/lecture_notes/notes4.pdf for diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch first means depth dimension is first in [n,m,z] notation\n",
    "\n",
    "Bi direcitonal means 2 directions\n",
    "https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66 -> input the sequence forward and backward, hidden is 1x for forward, 1x for backward 2x for output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
