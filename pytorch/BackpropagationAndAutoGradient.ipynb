{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g} \\frac{ \\partial g } {\\partial x} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac {\\partial loss}{\\partial x} $\n",
    "\n",
    "This is like saying, because of X, what is the rate of error\n",
    "\n",
    "Find the final loss, we can chain rule together all of the intermediate derivates (backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two inputs into a single neuron\n",
    "\n",
    "$x, y$ outputs a value $z$\n",
    "\n",
    "We want to find\n",
    "\n",
    "$ \\frac{\\partial loss}{\\partial x} \\frac{\\partial loss}{\\partial y} $\n",
    "\n",
    "This will tell us the rate of loss due to x and y which we want to be 0 ideally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, calculate\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial y}$\n",
    "\n",
    "which is the change of z due to y or x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, assume we get $\\frac{\\partial loss}{\\partial z}$\n",
    "\n",
    "$ \\frac{\\partial loss}{\\partial x} = \\frac{\\partial loss}{\\partial z} \\frac{\\partial z}{\\partial x} $ \n",
    "\n",
    "$ \\frac{\\partial loss}{\\partial y} = \\frac{\\partial loss}{\\partial z} \\frac{\\partial z}{\\partial y} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More concrete example\n",
    "\n",
    "x = 2\n",
    "y = 3\n",
    "\n",
    "Neuron function\n",
    "\n",
    "$f(x,y) = x * y$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x} = y$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial y} = x$\n",
    "\n",
    "Assume $\\frac{\\partial loss}{\\partial z} = 5$\n",
    "\n",
    "$ \\frac{\\partial loss}{\\partial x} = 3 * 5 = 15 $\n",
    "\n",
    "$ \\frac{\\partial loss}{\\partial y} = 2 * 5 = 10 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph\n",
    "\n",
    "$ loss = (xw - y) ^ 2 = (\\hat{y} - y) ^ 2 $\n",
    "\n",
    "$ \\hat{y} = xw $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xw = first gate\n",
    "\n",
    "$\\hat{y}$ - y = second gate\n",
    "\n",
    "$()^2$ = third gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward\n",
    "\n",
    "x = 1, y = 2, w = 1\n",
    "\n",
    "$\\hat{y} = 1$\n",
    "\n",
    "$s = 1 - y = -1$\n",
    "\n",
    "$loss = -1 ^ 2 = 1$ (We actually don't care about the loss here, we care about making the rate of error = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "\n",
    "$\\frac{\\partial s^2}{\\partial s} = 2s$ (Derivate of $s^2$ with respect to $s$)\n",
    "\n",
    "$\\frac{\\partial loss}{\\partial s} = 2s = -2$\n",
    "\n",
    "$\\frac{\\partial \\hat{y} - y}{\\partial \\hat{y}} = 1$\n",
    "\n",
    "$ s = \\hat{y} - y $\n",
    "\n",
    "$\\frac{\\partial loss}{\\partial \\hat{y}} = \\frac{\\partial loss}{\\partial s}\\frac{\\partial s}{\\partial \\hat{y}} = -2 * 1 = -2 $\n",
    "\n",
    "$\\frac{\\partial xw}{\\partial w} = x$\n",
    "\n",
    "$ \\hat{y} = xw $\n",
    "\n",
    "$\\frac{\\partial loss}{\\partial w} = \\frac{\\partial loss}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial w} = -2 * x = -2 * 1 = -2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4-1\n",
    "x = 2, y = 4, w =1\n",
    "\n",
    "$\\hat{y} = 2 = 2 * 1 = x * w$\n",
    "\n",
    "$s = \\hat{y} - y = 2 - 4 = -2$\n",
    "\n",
    "$s ^ 2 = -2 ^ 2 = 4 = loss$\n",
    "\n",
    "$2 * s = 2 * -2  = 2 * s  = -4$\n",
    "\n",
    "$ -4 * 1 = -4 $\n",
    "\n",
    "$ 2 * -4 = -8 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://my.pcloud.com/publink/show?code=XZYHWS7ZGNkSsHHaVhbDtGtgq8iMlY73yek0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When x=3, result is 3.0, should be 6\n",
      "\t x = 1.0, y = 2.0, w=$tensor([ 1.]), pred=$tensor([ 1.]), grad = -2.0\n",
      "\t x = 2.0, y = 4.0, w=$tensor([ 1.0200]), pred=$tensor([ 2.0400]), grad = -7.840000152587891\n",
      "\t x = 3.0, y = 6.0, w=$tensor([ 1.0984]), pred=$tensor([ 3.2952]), grad = -16.228801727294922\n",
      "episode 0 last loss = 7.315943717956543 (error^2)\n",
      "\t x = 1.0, y = 2.0, w=$tensor([ 1.2607]), pred=$tensor([ 1.2607]), grad = -1.478623867034912\n",
      "\t x = 2.0, y = 4.0, w=$tensor([ 1.2755]), pred=$tensor([ 2.5509]), grad = -5.796205520629883\n",
      "\t x = 3.0, y = 6.0, w=$tensor([ 1.3334]), pred=$tensor([ 4.0003]), grad = -11.998146057128906\n",
      "episode 1 last loss = 3.9987640380859375 (error^2)\n",
      "\t x = 1.0, y = 2.0, w=$tensor([ 1.4534]), pred=$tensor([ 1.4534]), grad = -1.0931644439697266\n",
      "\t x = 2.0, y = 4.0, w=$tensor([ 1.4643]), pred=$tensor([ 2.9287]), grad = -4.285204887390137\n",
      "\t x = 3.0, y = 6.0, w=$tensor([ 1.5072]), pred=$tensor([ 4.5216]), grad = -8.870372772216797\n",
      "episode 2 last loss = 2.1856532096862793 (error^2)\n",
      "\t x = 1.0, y = 2.0, w=$tensor([ 1.5959]), pred=$tensor([ 1.5959]), grad = -0.8081896305084229\n",
      "\t x = 2.0, y = 4.0, w=$tensor([ 1.6040]), pred=$tensor([ 3.2080]), grad = -3.1681032180786133\n",
      "\t x = 3.0, y = 6.0, w=$tensor([ 1.6357]), pred=$tensor([ 4.9070]), grad = -6.557973861694336\n",
      "episode 3 last loss = 1.1946394443511963 (error^2)\n",
      "\t x = 1.0, y = 2.0, w=$tensor([ 1.7012]), pred=$tensor([ 1.7012]), grad = -0.5975041389465332\n",
      "\t x = 2.0, y = 4.0, w=$tensor([ 1.7072]), pred=$tensor([ 3.4144]), grad = -2.3422164916992188\n",
      "\t x = 3.0, y = 6.0, w=$tensor([ 1.7306]), pred=$tensor([ 5.1919]), grad = -4.848389625549316\n",
      "episode 4 last loss = 0.6529689431190491 (error^2)\n",
      "\t x = 1.0, y = 2.0, w=$tensor([ 1.7791]), pred=$tensor([ 1.7791]), grad = -0.4417421817779541\n",
      "\t x = 2.0, y = 4.0, w=$tensor([ 1.7835]), pred=$tensor([ 3.5671]), grad = -1.7316293716430664\n",
      "\t x = 3.0, y = 6.0, w=$tensor([ 1.8009]), pred=$tensor([ 5.4026]), grad = -3.58447265625\n",
      "episode 5 last loss = 0.35690122842788696 (error^2)\n",
      "\t x = 1.0, y = 2.0, w=$tensor([ 1.8367]), pred=$tensor([ 1.8367]), grad = -0.3265852928161621\n",
      "\t x = 2.0, y = 4.0, w=$tensor([ 1.8400]), pred=$tensor([ 3.6799]), grad = -1.2802143096923828\n",
      "\t x = 3.0, y = 6.0, w=$tensor([ 1.8528]), pred=$tensor([ 5.5583]), grad = -2.650045394897461\n",
      "episode 6 last loss = 0.195076122879982 (error^2)\n",
      "\t x = 1.0, y = 2.0, w=$tensor([ 1.8793]), pred=$tensor([ 1.8793]), grad = -0.24144840240478516\n",
      "\t x = 2.0, y = 4.0, w=$tensor([ 1.8817]), pred=$tensor([ 3.7634]), grad = -0.9464778900146484\n",
      "\t x = 3.0, y = 6.0, w=$tensor([ 1.8912]), pred=$tensor([ 5.6735]), grad = -1.9592113494873047\n",
      "episode 7 last loss = 0.10662525147199631 (error^2)\n",
      "\t x = 1.0, y = 2.0, w=$tensor([ 1.9107]), pred=$tensor([ 1.9107]), grad = -0.17850565910339355\n",
      "\t x = 2.0, y = 4.0, w=$tensor([ 1.9125]), pred=$tensor([ 3.8251]), grad = -0.699742317199707\n",
      "\t x = 3.0, y = 6.0, w=$tensor([ 1.9195]), pred=$tensor([ 5.7586]), grad = -1.4484672546386719\n",
      "episode 8 last loss = 0.0582793727517128 (error^2)\n",
      "\t x = 1.0, y = 2.0, w=$tensor([ 1.9340]), pred=$tensor([ 1.9340]), grad = -0.1319713592529297\n",
      "\t x = 2.0, y = 4.0, w=$tensor([ 1.9353]), pred=$tensor([ 3.8707]), grad = -0.5173273086547852\n",
      "\t x = 3.0, y = 6.0, w=$tensor([ 1.9405]), pred=$tensor([ 5.8215]), grad = -1.070866584777832\n",
      "episode 9 last loss = 0.03185431286692619 (error^2)\n",
      "When x=3, result is 5.8536481857299805, should be 6\n"
     ]
    }
   ],
   "source": [
    "w = Variable(torch.Tensor([1.0]), requires_grad=True)\n",
    "print(f\"When x=3, result is {forward(3).data[0]}, should be 6\")\n",
    "for ep in range(10):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        err = loss(x, y)\n",
    "        err.backward()\n",
    "        print(f'\\t x = {x}, y = {y}, w=${w.data}, pred=${forward(x)}, grad = {w.grad.data[0]}')\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        w.grad.data.zero_()\n",
    "    print(f\"episode {ep} last loss = {err.data[0]} (error^2)\")\n",
    "print(f\"When x=3, result is {forward(3).data[0]}, should be 6\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo 4-2 / 4-3 / 4-4 / 4-5 Exercise for Fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
